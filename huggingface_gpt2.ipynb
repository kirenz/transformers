{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace GPT-2 Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HuggingFace provides a wide range of pre-trained transformer models that are readily available for use.\n",
    "\n",
    "- These models are trained on large amounts of data and fine-tuned for specific tasks, making them highly effective in various NLP applications.\n",
    "\n",
    "- With HuggingFace's pretrained transformer models, we can quickly prototype and deploy state-of-the-art models without the need for extensive training on massive datasets.\n",
    "\n",
    "- The GPT-2 model was developed by OpenAI, but it is now publicly available through the Hugging Face Transformers library.\n",
    "\n",
    "- OpenAI released several versions of the GPT-2 model with varying sizes and capabilities, ranging from the 117M parameter model to the 1.5B parameter model. The pre-trained GPT-2 models are available for download from the Hugging Face model hub and can be loaded using the TFGPT2LMHeadModel or GPT2LMHeadModel classes in the Transformers library.\n",
    "\n",
    "\n",
    "To learn more, take a look at the [HuggingFace website](https://huggingface.co/docs/transformers/index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model_config = GPT2Config.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love to see what you're doing at your local gym, and that's all it is! I love to see what you're working on, and that's all it is! It's so fun. Thank you so much! It's so good to see\n"
     ]
    }
   ],
   "source": [
    "# Update the model config for text generation\n",
    "model_config.max_new_tokens = 50\n",
    "model_config.do_sample = True\n",
    "model_config.top_p = 0.95\n",
    "model_config.top_k = 50\n",
    "\n",
    "# Load the GPT-2 model with the updated config\n",
    "model = TFGPT2LMHeadModel.from_pretrained('gpt2', config=model_config)\n",
    "\n",
    "# Define the input text to generate from\n",
    "input_text = \"I love to\"\n",
    "\n",
    "# Tokenize the input text using the tokenizer\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
    "\n",
    "# Generate text using the pre-trained GPT-2 model\n",
    "output_ids = model.generate(\n",
    "    input_ids,                            # Input IDs for the model\n",
    "    pad_token_id=tokenizer.eos_token_id,  # ID of the end-of-string token\n",
    "    attention_mask=tf.ones(input_ids.shape, dtype=tf.int32) # Attention mask to indicate which tokens to attend to\n",
    ")\n",
    "\n",
    "# Decode the output text from the output IDs using the tokenizer\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated output text\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what each step does:\n",
    "\n",
    "- Import the necessary libraries, including TensorFlow and the Hugging Face Transformers library.\n",
    "\n",
    "- Load the pre-trained GPT-2 model and tokenizer using the GPT2Tokenizer.from_pretrained and GPT2Config.from_pretrained methods from the Transformers library.\n",
    "\n",
    "- Update the model config for text generation by setting the max_new_tokens, do_sample, top_p, and top_k attributes of the model_config object.\n",
    "\n",
    "- Load the GPT-2 model with the updated config by calling the TFGPT2LMHeadModel.from_pretrained method with the gpt2 model name and the model_config object.\n",
    "\n",
    "- Define the input text to generate from.\n",
    "\n",
    "- Tokenize the input text using the encode method of the tokenizer.\n",
    "\n",
    "- Generate text using the pre-trained GPT-2 model by calling the generate method of the model with the input IDs, the end-of-string token ID, and an attention mask to indicate which tokens to attend to.\n",
    "\n",
    "- Decode the output text from the output IDs using the decode method of the tokenizer.\n",
    "\n",
    "- Print the generated output text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
