{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace GPT-2 Transformer with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the necessary libraries, including TensorFlow and the Hugging Face Transformers library.\n",
    "\n",
    "- Import your config file with your HuggingFace API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the pre-trained GPT-2 model and tokenizer from the Transformers library.\n",
    "\n",
    "- A *tokenizer* is a tool used in natural language processing (NLP) to split text into smaller units, such as words or subwords, that can be more easily processed by a machine learning model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', api_key=config.API_KEY)\n",
    "model_config = GPT2Config.from_pretrained('gpt2', api_key=config.API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Update the model config for text generation by setting the max_new_tokens, do_sample, top_p, and top_k attributes of the model_config object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the model config for text generation\n",
    "model_config.max_new_tokens = 50\n",
    "model_config.do_sample = True\n",
    "model_config.top_p = 0.95\n",
    "model_config.top_k = 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `max_new_tokens`: This option specifies the maximum number of new tokens that the model can generate when generating new text. By default, the value of max_new_tokens is set to 50, which means that the model can generate up to 50 new tokens in each generation step. This option can be useful for controlling the length of the generated text and preventing the model from generating very long or very short outputs.\n",
    "\n",
    "- `do_sample`: This option is a boolean flag that controls whether or not the model uses sampling during text generation. If do_sample is set to True, the model uses sampling to randomly select the next token from the predicted probability distribution at each generation step. If do_sample is set to False, the model always chooses the token with the highest probability at each generation step (i.e., greedy decoding).\n",
    "\n",
    "- `top_p`: This option controls the \"nucleus\" sampling strategy during text generation. The top_p value specifies the cumulative probability mass of the most likely tokens to consider. For example, if top_p is set to 0.95, the model will consider the most likely tokens that, taken together, represent 95% of the probability mass of the predicted probability distribution.\n",
    "\n",
    "- `top_k`: This option controls the \"top-k\" sampling strategy during text generation. The top_k value specifies the number of most likely tokens to consider at each generation step. For example, if top_k is set to 50, the model will consider the 50 most likely tokens from the predicted probability distribution at each generation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference between top_k and top_p is that top_k limits the number of possible next tokens to a fixed number, whereas top_p limits the number of possible next tokens based on their probability mass. top_k can be useful for generating more diverse text by preventing the model from always choosing the most likely token, whereas top_p can be useful for controlling the diversity and quality of the generated text by allowing the model to choose from a smaller set of more likely tokens."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the GPT-2 model with the updated config by calling the `TFGPT2LMHeadModel.from_pretrained` method with the gpt2 model name and the model_config object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load the GPT-2 model with the updated config\n",
    "model = TFGPT2LMHeadModel.from_pretrained('gpt2', config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the input text to generate from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input text to generate from\n",
    "input_text = \"I love to\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenize the input text using the encode method of the tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input text using the tokenizer\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generate text using the pre-trained GPT-2 model by calling the generate method of the model with the input IDs, the end-of-string token ID, and an attention mask to indicate which tokens to attend to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jankirenz/miniconda3/envs/transformers-env/lib/python3.10/site-packages/transformers/generation/tf_utils.py:745: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Generate text using the pre-trained GPT-2 model\n",
    "output_ids = model.generate(\n",
    "    input_ids,                            # Input IDs for the model\n",
    "    pad_token_id=tokenizer.eos_token_id,  # ID of the end-of-string token\n",
    "    attention_mask=tf.ones(input_ids.shape, dtype=tf.int32) # Attention mask to indicate which tokens to attend to\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decode the output text from the output IDs using the decode method of the tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the output text from the output IDs using the tokenizer\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print the generated output text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love to have these guys play with me now and they always make it work. You never know.\"\n",
      "\n",
      "The Knicks (29-29) won their first 14 games of the season before losing to the Suns, 84-85. They are 7-8\n"
     ]
    }
   ],
   "source": [
    "# Print the generated output text\n",
    "print(output_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
